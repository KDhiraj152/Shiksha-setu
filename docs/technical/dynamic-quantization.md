# Dynamic Quantization System

## Overview

The Dynamic Quantization Manager provides **on-demand resource allocation** with variable quantization levels that adapt based on:

- **System Memory**: Available RAM and memory pressure
- **Node Load**: Number of concurrent inference requests  
- **Device Capabilities**: MPS/CUDA/CPU support for different precision levels
- **Task Priority**: Quality vs speed requirements

This prevents bottlenecks by automatically adjusting model precision to match available resources.

---

## Quantization Levels

### FP16 (Half Precision)
- **Compression**: 2x (14GB ‚Üí 7GB for 7B model)
- **Quality**: Best (minimal degradation)
- **Speed**: Fast on GPU/MPS
- **Use Case**: When memory is comfortable (<40% used) and load is low

### INT8 (8-bit Quantization)
- **Compression**: 4x (14GB ‚Üí 3.5GB for 7B model)
- **Quality**: Excellent (slight degradation)
- **Speed**: Very fast
- **Use Case**: Moderate memory pressure (40-60% used)

### INT4 (4-bit Quantization)
- **Compression**: 8x (14GB ‚Üí 1.75GB for 7B model)
- **Quality**: Good (noticeable but acceptable)
- **Speed**: Fast with optimized kernels
- **Use Case**: High memory pressure (60-75% used) or moderate load

### INT2 (2-bit GGUF Quantization)
- **Compression**: 16x (14GB ‚Üí 875MB for 7B model)
- **Quality**: Acceptable for simple tasks
- **Speed**: Fastest inference
- **Use Case**: Critical memory pressure (>85% used) or high load (>10 requests)

---

## Adaptive Algorithm

### Decision Matrix

```
Memory Usage | Load (Requests) | Selected Level
-------------|-----------------|---------------
< 40%        | < 2             | FP16 (best quality)
< 40%        | 2-5             | INT8 (balanced)
40-60%       | < 2             | INT8 (balanced)
40-60%       | 2-5             | INT4 (efficient)
60-75%       | Any             | INT4 (required)
75-85%       | < 10            | INT4 (critical)
> 85%        | Any             | INT2 (emergency)
```

### Task Priority Override

- **Quality Priority**: Prefers higher precision (FP16 > INT8 > INT4)
- **Balanced**: Uses decision matrix as-is
- **Speed Priority**: Prefers lower precision for faster inference

---

## API Endpoints

### GET /api/v1/quantization/status

Get current quantization status and system metrics.

**Response:**
```json
{
  "device": "mps",
  "active_requests": 3,
  "memory": {
    "total_gb": 16.0,
    "available_gb": 8.5,
    "used_percent": "53.1%",
    "process_gb": 4.2
  },
  "capabilities": {
    "fp16": true,
    "int8": false,
    "int4": true
  },
  "recommended_level": "int4",
  "current_models": [
    {
      "name": "qwen-7b",
      "size_mb": 3500,
      "access_count": 45,
      "status": "loaded"
    }
  ]
}
```

### POST /api/v1/quantization/calculate

Calculate optimal quantization for specific parameters.

**Request:**
```json
{
  "model_size_params": 7.0,
  "task_priority": "balanced"
}
```

**Response:**
```json
{
  "level": "int4",
  "precision": "int4",
  "compression_ratio": "8.0x",
  "estimated_memory_gb": 1.75,
  "config": {
    "load_in_4bit": true,
    "bnb_4bit_compute_dtype": "float16",
    "bnb_4bit_quant_type": "nf4"
  }
}
```

### GET /api/v1/quantization/memory

Get detailed memory metrics.

**Response:**
```json
{
  "total_gb": 16.0,
  "available_gb": 8.5,
  "used_gb": 7.5,
  "percent_used": "46.9%",
  "process_memory_gb": 4.2,
  "status": "moderate"
}
```

### GET /api/v1/quantization/cache

Get model cache statistics.

**Response:**
```json
{
  "current_size_mb": 3500,
  "max_size_mb": 8000,
  "utilization": "43.8%",
  "models_cached": 1,
  "models": [...]
}
```

### POST /api/v1/quantization/cache/clear

Clear model cache to free memory.

**Response:**
```json
{
  "status": "success",
  "message": "Cache cleared"
}
```

---

## Usage Examples

### Automatic (Recommended)

The system automatically adjusts quantization based on load:

```python
from backend.services.unified_model_client import get_unified_client

client = get_unified_client()

# System automatically selects optimal quantization
result = await client.simplify_text(
    text="Complex content...",
    grade_level=8,
    subject="Science"
)
```

### Force Specific Level

Override automatic selection for testing or specific requirements:

```python
from backend.core.model_loader import get_model_loader

loader = get_model_loader()

# Force FP16 (best quality)
model = loader.load_model(
    "qwen-7b",
    model_size_params=7.0,
    force_quantization="fp16"
)

# Force INT4 (memory efficient)
model = loader.load_model(
    "qwen-7b", 
    model_size_params=7.0,
    force_quantization="int4"
)
```

### Task Priority

Influence quantization selection:

```python
# Prioritize quality (prefers higher precision)
result = await client.simplify_text(
    text="Complex content...",
    grade_level=12,
    subject="Mathematics",
    task_priority="quality"
)

# Prioritize speed (prefers lower precision)
result = await client.simplify_text(
    text="Simple content...",
    grade_level=5,
    subject="English",
    task_priority="speed"
)
```

---

## Dynamic Re-optimization

The system continuously monitors resources and can re-optimize loaded models:

```python
# Initial load at FP16 (memory comfortable)
model = loader.load_model("qwen-7b", model_size_params=7.0)
# Loaded: FP16, 7GB memory

# ... multiple requests increase load, memory pressure rises ...

# Next request triggers re-optimization
model = loader.load_model("qwen-7b", model_size_params=7.0)
# Detected: memory > 60% used
# Reloaded: INT4, 1.75GB memory (freed 5.25GB)
```

---

## Performance Impact

### Latency (7B Model, Apple M4)

| Level | First Token | Tokens/sec | Quality Score |
|-------|-------------|------------|---------------|
| FP16  | 200ms       | 45 tok/s   | 0.95          |
| INT8  | 180ms       | 52 tok/s   | 0.93          |
| INT4  | 150ms       | 65 tok/s   | 0.88          |
| INT2  | 120ms       | 80 tok/s   | 0.75          |

### Memory Usage (7B Model)

| Level | Model Size | Peak Memory | Compression |
|-------|-----------|-------------|-------------|
| FP16  | 14GB      | 16GB        | 1x          |
| INT8  | 7GB       | 9GB         | 2x          |
| INT4  | 3.5GB     | 5GB         | 4x          |
| INT2  | 1.75GB    | 3GB         | 8x          |

---

## Monitoring

### CLI Monitoring

```bash
# Watch quantization status
watch -n 2 'curl -s http://localhost:8000/api/v1/quantization/status | jq'

# Monitor memory
watch -n 1 'curl -s http://localhost:8000/api/v1/quantization/memory | jq'

# Check cache
curl http://localhost:8000/api/v1/quantization/cache | jq
```

### Dashboard Integration

Add to frontend monitoring dashboard:

```typescript
// Fetch quantization status
const response = await fetch('/api/v1/quantization/status');
const status = await response.json();

// Display memory gauge
<MemoryGauge 
  used={status.memory.used_percent}
  level={status.recommended_level}
/>

// Show active models
<ModelCache models={status.current_models} />
```

---

## Configuration

### Environment Variables

```bash
# Maximum model cache size (MB)
MAX_MODEL_CACHE_MB=8000

# Memory thresholds (0.0-1.0)
QUANTIZE_MEMORY_COMFORTABLE=0.4
QUANTIZE_MEMORY_MODERATE=0.6
QUANTIZE_MEMORY_TIGHT=0.75
QUANTIZE_MEMORY_CRITICAL=0.85

# Load thresholds (concurrent requests)
QUANTIZE_LOAD_LOW=2
QUANTIZE_LOAD_MODERATE=5
QUANTIZE_LOAD_HIGH=10
```

### Python Configuration

```python
from backend.core.dynamic_quantization import DynamicQuantizationManager

# Custom thresholds
manager = DynamicQuantizationManager(device="mps")
manager.MEMORY_COMFORTABLE = 0.3  # More aggressive
manager.LOAD_HIGH = 15            # Higher load threshold
```

---

## Best Practices

### 1. Let it Auto-Optimize
- Don't force quantization unless testing
- System adapts better than manual tuning
- Monitor via API to understand behavior

### 2. Set Appropriate Priorities
- Use `"quality"` for grade 11-12 or complex subjects
- Use `"balanced"` for most tasks (default)
- Use `"speed"` for grade 5-7 or simple content

### 3. Monitor Memory Trends
- Check `/quantization/memory` regularly
- Clear cache if memory stays >80%
- Consider upgrading hardware if consistently >85%

### 4. Handle Re-optimization
- Expect occasional latency spikes during re-optimization
- System prioritizes availability over consistency
- Cache warming helps reduce frequency

### 5. Test Different Loads
- Simulate concurrent requests in testing
- Verify quantization degrades gracefully
- Check quality doesn't drop below acceptable threshold

---

## Troubleshooting

### Issue: Memory keeps growing

**Solution:**
```bash
# Clear cache manually
curl -X POST http://localhost:8000/api/v1/quantization/cache/clear

# Check for memory leaks
curl http://localhost:8000/api/v1/quantization/memory
```

### Issue: Quality degraded too much

**Force higher precision:**
```python
# Override to INT8 minimum
result = await client.simplify_text(
    text=content,
    grade_level=12,
    subject="Mathematics",
    task_priority="quality",
    min_quantization="int8"  # Don't go below INT8
)
```

### Issue: Models not re-optimizing

**Check thresholds:**
```python
manager = get_quantization_manager()
print(manager.get_status())

# Memory might not have crossed threshold yet
# Or load might still be low
```

---

## Architecture

### Flow Diagram

```
Request ‚Üí UnifiedModelClient
            ‚Üì
         Check Cache
            ‚Üì
         Cache Miss
            ‚Üì
    DynamicQuantizationManager
            ‚Üì
    Calculate Optimal Level (based on memory + load)
            ‚Üì
         ModelLoader
            ‚Üì
    Load with Quantization
            ‚Üì
         Cache Model
            ‚Üì
    Return for Inference
```

### Components

1. **DynamicQuantizationManager**: Monitors system, calculates optimal levels
2. **ModelLoader**: Loads models with specified quantization
3. **UnifiedModelClient**: Orchestrates inference with dynamic quantization
4. **ModelCache**: LRU cache with automatic eviction
5. **Quantization API**: REST endpoints for monitoring/control

---

## Future Enhancements

- [ ] Per-model quantization profiles
- [ ] Automatic A/B testing of quantization levels
- [ ] Quality feedback loop (adjust based on validation scores)
- [ ] Mixed precision (different layers at different precisions)
- [ ] Predictive pre-loading based on traffic patterns
- [ ] Multi-GPU load balancing with different quantizations

---

## References

- BitsAndBytes: https://github.com/TimDettmers/bitsandbytes
- llama.cpp GGUF: https://github.com/ggerganov/llama.cpp
- PyTorch Quantization: https://pytorch.org/docs/stable/quantization.html
- Apple MPS Backend: https://pytorch.org/docs/stable/notes/mps.html

---

## üë®‚Äçüíª Author

**K Dhiraj** ‚Ä¢ [k.dhiraj.srihari@gmail.com](mailto:k.dhiraj.srihari@gmail.com) ‚Ä¢ [@KDhiraj152](https://github.com/KDhiraj152) ‚Ä¢ [LinkedIn](https://www.linkedin.com/in/k-dhiraj-83b025279/)

*Last updated: November 2025*
