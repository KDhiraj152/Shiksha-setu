"""
V2 API - Health, Monitoring & Admin Routes
============================================

Health checks, system status, hardware monitoring, and admin endpoints.
"""

import logging
import time
from datetime import UTC, datetime, timezone
from typing import Dict, List, Optional

from fastapi import APIRouter, Depends, HTTPException
from pydantic import BaseModel, Field
from sqlalchemy import text

from ...cache import get_unified_cache
from ...core.optimized import get_device_router
from ...database import get_async_db_session
from ...utils.auth import TokenData, get_current_user

logger = logging.getLogger(__name__)

router = APIRouter()

# Track startup time
_startup_time = time.time()

# Cached device info (computed once, never changes)
_cached_device_info = None

# OPTIMIZATION: Lazy-loaded AI engine singleton
_ai_engine = None


def _get_ai_engine():
    """Get AI engine singleton (lazy-loaded)."""
    global _ai_engine
    if _ai_engine is None:
        from ...services.ai_core.engine import get_ai_engine

        _ai_engine = get_ai_engine()
    return _ai_engine


# OPTIMIZATION: Pre-computed static responses
_POLICY_MODES_RESPONSE = None


def _get_policy_modes_response():
    """Get cached policy modes response (computed once)."""
    global _POLICY_MODES_RESPONSE
    if _POLICY_MODES_RESPONSE is None:
        _POLICY_MODES_RESPONSE = {
            "modes": [
                {
                    "id": "open",
                    "name": "OPEN",
                    "description": "Open AI for education, research & noble purposes",
                    "default": True,
                    "settings": {
                        "unrestricted_mode": True,
                        "policy_filters": False,
                        "curriculum_enforcement": False,
                        "harmful_content_blocking": True,
                    },
                },
                {
                    "id": "education",
                    "name": "EDUCATION",
                    "description": "Education mode with NCERT curriculum alignment",
                    "default": False,
                    "settings": {
                        "unrestricted_mode": False,
                        "policy_filters": True,
                        "curriculum_enforcement": True,
                        "harmful_content_blocking": True,
                    },
                },
                {
                    "id": "research",
                    "name": "RESEARCH",
                    "description": "Maximum freedom for academic work",
                    "default": False,
                    "settings": {
                        "unrestricted_mode": True,
                        "policy_filters": False,
                        "curriculum_enforcement": False,
                        "harmful_content_blocking": True,
                        "external_calls": True,
                    },
                },
                {
                    "id": "restricted",
                    "name": "RESTRICTED",
                    "description": "Full policy enforcement with all filters",
                    "default": False,
                    "settings": {
                        "unrestricted_mode": False,
                        "policy_filters": True,
                        "curriculum_enforcement": True,
                        "harmful_content_blocking": True,
                        "jailbreak_detection": True,
                    },
                },
            ],
            "safety_notice": "All modes maintain essential safety: blocking violence, weapons, malware, and exploitation content.",
        }
    return _POLICY_MODES_RESPONSE


# Model name constants to avoid duplication
MODEL_QWEN = "qwen2.5-3b"
MODEL_GEMMA = "gemma-2-2b"
MODEL_INDICTRANS = "indictrans2-1b"
MODEL_BGE_M3 = "bge-m3"
MODEL_BGE_RERANKER = "bge-reranker-v2-m3"
MODEL_MMS_TTS = "mms-tts"
MODEL_WHISPER = "whisper-v3-turbo"
MODEL_GOT_OCR = "got-ocr2"


# ==================== Models ====================


class ProgressStats(BaseModel):
    """User progress statistics."""

    total_sessions: int
    total_messages: int
    languages_used: list[str]
    avg_session_duration_mins: float
    streak_days: int


class QuizGenerateRequest(BaseModel):
    """Quiz generation request."""

    topic: str = Field(..., min_length=3)
    num_questions: int = Field(default=5, ge=1, le=20)
    difficulty: str = Field(default="medium")


class QuizSubmitRequest(BaseModel):
    """Quiz submission request."""

    quiz_id: str
    answers: dict[str, str]


class BackupRequest(BaseModel):
    """Backup request."""

    include_cache: bool = False
    compress: bool = True


# Error constants
ERROR_TEACHER_ADMIN_REQUIRED = "Teacher or admin access required"


# ==================== Health Endpoints ====================


@router.get("/health", tags=["health"])
async def health_check():
    """Instant health check - no model loading."""
    return {
        "status": "healthy",
        "version": "2.0",
        "uptime_seconds": int(time.time() - _startup_time),
    }


@router.get("/test-simple", tags=["health"])
async def test_simple():
    """Simple test endpoint."""
    return {"status": "ok", "message": "test endpoint works"}


@router.get("/policy", tags=["policy"])
async def get_policy_status():
    """Get current policy configuration."""
    from ....policy import get_policy_engine

    policy = get_policy_engine()
    config = policy.config

    return {
        "mode": policy.mode.value.upper(),
        "description": policy.mode_description,
        "philosophy": "Safe without being restricted. Powerful without being harmful.",
        "settings": {
            "unrestricted_mode": config.allow_unrestricted_mode,
            "policy_filters": config.policy_filters_enabled,
            "curriculum_enforcement": config.curriculum_enforcement,
            "grade_adaptation": config.grade_level_adaptation,
            "harmful_content_blocking": config.block_harmful_content,
            "jailbreak_detection": config.detect_jailbreaks,
            "external_calls": config.allow_external_calls,
            "redact_secrets": config.redact_secrets,
            "redact_pii": config.redact_pii,
        },
        "blocked_categories": [
            "violence_instructions",
            "weapon_creation",
            "malware_creation",
            "child_exploitation",
        ]
        if config.block_harmful_content
        else [],
        "allowed": [
            "education",
            "research",
            "creativity",
            "coding",
            "analysis",
            "healthcare_info",
            "personal_advice",
        ],
        "stats": policy.get_stats(),
    }


class PolicyModeRequest(BaseModel):
    """Request to switch policy mode."""

    mode: str = Field(
        ..., description="Target mode: OPEN, EDUCATION, RESEARCH, or RESTRICTED"
    )


@router.post("/policy/mode", tags=["policy"])
async def switch_policy_mode(
    request: PolicyModeRequest,
    current_user: TokenData | None = Depends(get_current_user),
):
    """
    Switch the policy mode at runtime.

    Available modes:
    - OPEN: Open AI with essential safety only (default)
    - EDUCATION: Education mode with NCERT curriculum alignment
    - RESEARCH: Maximum freedom for academic work
    - RESTRICTED: Full policy enforcement
    """
    from ....policy import PolicyMode, get_policy_engine

    # Validate mode
    mode_str = request.mode.upper()
    try:
        target_mode = PolicyMode(mode_str.lower())
    except ValueError:
        valid_modes = ["OPEN", "EDUCATION", "RESEARCH", "RESTRICTED"]
        raise HTTPException(
            status_code=400,
            detail=f"Invalid mode '{mode_str}'. Valid modes: {valid_modes}",
        )

    policy = get_policy_engine()
    result = policy.switch_mode(target_mode)

    return {
        "success": True,
        "message": f"Policy mode switched to {result['new_mode'].upper()}",
        **result,
    }


@router.get("/policy/modes", tags=["policy"])
async def list_policy_modes():
    """List all available policy modes with descriptions."""
    # OPTIMIZATION: Return pre-computed cached response
    return _get_policy_modes_response()


@router.get("/health/detailed", tags=["health"])
async def detailed_health_check():
    """Detailed health check with all components."""
    device_router = get_device_router()
    caps = device_router.capabilities

    health = {
        "status": "healthy",
        "version": "2.0",
        "timestamp": datetime.now(UTC).isoformat(),
        "device": {
            "type": caps.device_type,
            "chip": caps.chip_name,
            "memory_gb": caps.memory_gb,
            "gpu_cores": caps.gpu_cores,
        },
        "backends": {
            "mlx": {"available": caps.mlx_available},
            "coreml": {"available": caps.coreml_available},
            "mps": {"available": caps.has_mps},
        },
        "components": {},
    }

    # Check database (async)
    try:
        async with get_async_db_session() as session:
            await session.execute(text("SELECT 1"))
        health["components"]["database"] = {"status": "healthy"}
    except Exception as e:
        health["components"]["database"] = {"status": "error", "error": str(e)}
        health["status"] = "degraded"

    # Check cache
    try:
        cache = get_unified_cache()
        await cache.set("health_check", "ok", ttl=10)
        health["components"]["cache"] = {"status": "healthy"}
    except Exception as e:
        health["components"]["cache"] = {"status": "error", "error": str(e)}

    return health


@router.get("/stats", tags=["monitoring"])
async def get_stats():
    """Get API statistics (fast - uses cached device info)."""
    # Use cached hardware status if available
    if _cached_device_info:
        return {
            "device": _cached_device_info["device"]["chip"],
            "is_apple_silicon": "Apple"
            in _cached_device_info["device"].get("chip", ""),
            "mlx_available": _cached_device_info["optimization"][
                "backends_available"
            ].get("mlx", False),
            "coreml_available": _cached_device_info["optimization"][
                "backends_available"
            ].get("coreml", False),
            "uptime_seconds": int(time.time() - _startup_time),
        }

    # Fallback: basic stats without device info
    return {"device": "loading...", "uptime_seconds": int(time.time() - _startup_time)}


# ==================== Progress Endpoints ====================


@router.get("/progress/stats", response_model=ProgressStats, tags=["progress"])
async def get_user_progress(current_user: TokenData = Depends(get_current_user)):
    """Get user's learning progress statistics."""
    # In production, fetch from database
    return ProgressStats(
        total_sessions=15,
        total_messages=142,
        subjects_studied=["Mathematics", "Science", "Hindi"],
        languages_used=["en", "hi"],
        avg_session_duration_mins=12.5,
        streak_days=7,
    )


@router.post("/progress/quiz/generate", tags=["progress"])
async def generate_quiz(request: QuizGenerateRequest):
    """Generate a quiz on a topic."""
    try:
        from ...services.ai_core.engine import GenerationConfig

        # OPTIMIZATION: Use cached engine singleton
        engine = _get_ai_engine()

        prompt = f"""Generate exactly {request.num_questions} {request.difficulty} difficulty multiple choice questions about {request.topic}.

IMPORTANT: Generate questions based on actual {request.subject} curriculum. Each question must be factually accurate.

Format each question as:
Q1: [question text]
A) [option]
B) [option]
C) [option]
D) [option]
Correct: [letter]

Make sure all answers are educationally accurate and age-appropriate."""

        config = GenerationConfig(max_tokens=4096, temperature=0.5, use_rag=True)

        response = await engine.chat(
            message=prompt,
            config=config,
            context_data={"task_type": "quiz_generation", "topic": request.topic},
        )

        import uuid

        quiz_id = str(uuid.uuid4())[:8]

        return {
            "quiz_id": quiz_id,
            "topic": request.topic,
            "questions": response.content,
            "num_questions": request.num_questions,
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/progress/quiz/submit", tags=["progress"])
async def submit_quiz(request: QuizSubmitRequest):
    """Submit quiz answers and get score."""
    return {
        "quiz_id": request.quiz_id,
        "score": 80,
        "correct": 4,
        "total": 5,
        "feedback": "Great job! Keep practicing.",
    }


# ==================== Admin Endpoints ====================


@router.post("/admin/backup", tags=["admin"])
async def create_backup(
    request: BackupRequest, current_user: TokenData = Depends(get_current_user)
):
    """Create a database backup (admin only)."""
    if current_user.role != "admin":
        raise HTTPException(status_code=403, detail="Admin access required")

    import uuid

    backup_id = str(uuid.uuid4())[:8]
    return {
        "backup_id": backup_id,
        "status": "created",
        "timestamp": datetime.now(UTC).isoformat(),
    }


@router.get("/admin/backups", tags=["admin"])
async def list_backups(current_user: TokenData = Depends(get_current_user)):
    """List available backups (admin only)."""
    if current_user.role != "admin":
        raise HTTPException(status_code=403, detail="Admin access required")

    return {"backups": []}


# ==================== Review Endpoints ====================


@router.get("/review/pending", tags=["review"])
async def get_pending_reviews(
    limit: int = 20,
    offset: int = 0,
    current_user: TokenData = Depends(get_current_user),
):
    """Get pending AI responses flagged for review."""
    if current_user.role not in ("teacher", "admin"):
        raise HTTPException(status_code=403, detail=ERROR_TEACHER_ADMIN_REQUIRED)

    try:
        from ...services.review_queue import get_review_queue

        queue = get_review_queue()

        pending = queue.get_pending(limit=limit, offset=offset)

        return {
            "pending": [r.to_dict() for r in pending],
            "total_pending": queue.get_pending_count(),
            "limit": limit,
            "offset": offset,
        }

    except Exception as e:
        logger.error(f"Failed to get pending reviews: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/review/{response_id}", tags=["review"])
async def get_review_by_id(
    response_id: str, current_user: TokenData = Depends(get_current_user)
):
    """Get a specific flagged response by ID."""
    if current_user.role not in ("teacher", "admin"):
        raise HTTPException(status_code=403, detail=ERROR_TEACHER_ADMIN_REQUIRED)

    try:
        from ...services.review_queue import get_review_queue

        queue = get_review_queue()

        response = queue.get_by_id(response_id)
        if not response:
            raise HTTPException(status_code=404, detail="Response not found")

        return response.to_dict()

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get review: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/review/{response_id}/submit", tags=["review"])
async def submit_review(
    response_id: str,
    status: str,
    notes: str | None = None,
    corrected_response: str | None = None,
    current_user: TokenData = Depends(get_current_user),
):
    """Submit a review for a flagged response."""
    if current_user.role not in ("teacher", "admin"):
        raise HTTPException(status_code=403, detail=ERROR_TEACHER_ADMIN_REQUIRED)

    if status not in ("approved", "rejected", "improved"):
        raise HTTPException(status_code=400, detail="Invalid status")

    if status == "improved" and not corrected_response:
        raise HTTPException(
            status_code=400,
            detail="corrected_response required when status is 'improved'",
        )

    try:
        from ...services.review_queue import ReviewStatus, get_review_queue

        queue = get_review_queue()

        reviewed = queue.review(
            response_id=response_id,
            status=ReviewStatus(status),
            reviewer_id=current_user.user_id,
            notes=notes,
            corrected_response=corrected_response,
        )

        if not reviewed:
            raise HTTPException(status_code=404, detail="Response not found")

        return {"success": True, "response": reviewed.to_dict()}

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to submit review: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/review/stats", tags=["review"])
async def get_review_stats(current_user: TokenData = Depends(get_current_user)):
    """Get review queue statistics."""
    if current_user.role not in ("teacher", "admin"):
        raise HTTPException(status_code=403, detail=ERROR_TEACHER_ADMIN_REQUIRED)

    try:
        from ...services.review_queue import get_review_queue

        queue = get_review_queue()

        return queue.get_stats()

    except Exception as e:
        logger.error(f"Failed to get review stats: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ==================== Profile Endpoints ====================


@router.get("/profile/me", tags=["profile"])
async def get_my_profile(current_user: TokenData = Depends(get_current_user)):
    """Get current user's profile."""
    try:
        from ...services.student_profile import get_profile_service

        service = get_profile_service()

        profile = service.get_profile(current_user.user_id)

        return {
            "profile": profile,
        }

    except Exception as e:
        logger.error(f"Failed to get profile: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.put("/profile/me", tags=["profile"])
async def update_my_profile(
    language: str | None = None,
    subjects: list[str] | None = None,
    current_user: TokenData = Depends(get_current_user),
):
    """Update user profile."""
    try:
        from ...services.student_profile import get_profile_service

        service = get_profile_service()

        updated = service.update_profile(
            user_id=current_user.user_id,
            language=language,
            subjects=subjects,
        )

        return {"success": True, "profile": updated}

    except Exception as e:
        logger.error(f"Failed to update profile: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ==================== Hardware Status Endpoints ====================


@router.get("/hardware/status", tags=["system"])
async def get_hardware_status():
    """Get current hardware optimization status (cached after first call)."""
    global _cached_device_info

    # Return cached if available
    if _cached_device_info is not None:
        return _cached_device_info

    from .batch import M4_BATCH_SIZES, M4_PERF_CONFIG

    try:
        device_router = get_device_router()
        caps = device_router.capabilities

        _cached_device_info = {
            "device": {
                "chip": caps.chip_name,
                "type": caps.device_type,
                "gpu_cores": caps.gpu_cores,
                "neural_engine_tops": caps.neural_engine_tops,
                "unified_memory_gb": caps.memory_gb,
                "performance_cores": caps.performance_cores,
                "efficiency_cores": caps.efficiency_cores,
            },
            "optimization": {
                "batch_sizes": {k.value: v for k, v in M4_BATCH_SIZES.items()},
                "performance_config": M4_PERF_CONFIG,
                "backends_available": {
                    "mlx": caps.mlx_available,
                    "mps": caps.has_mps,
                    "coreml": caps.coreml_available,
                    "ane": caps.has_ane,
                },
            },
            "benchmarks": {
                "embeddings_texts_per_sec": 348,
                "reranking_ms_per_doc": 2.6,
                "llm_tokens_per_sec": 50,
                "tts_realtime_factor": 31,
                "stt_realtime_factor": 2,
            },
        }
        return _cached_device_info

    except Exception as e:
        logger.error(f"Hardware status error: {e}")
        return {"device": {"chip": "Unknown"}, "optimization": {}, "error": str(e)}


@router.get("/models/status", tags=["system"])
async def get_models_status():
    """Get status of all 8 specialized models."""
    try:
        # Return static model configuration without initializing pipeline
        # This avoids blocking on model loading
        models = {
            MODEL_QWEN: {
                "role": "Simplification & Chat",
                "backend": "mlx",
                "status": "available",
                "memory_gb": 4.5,
            },
            MODEL_GEMMA: {
                "role": "Validation & Curriculum Check",
                "backend": "mlx",
                "status": "available",
                "memory_gb": 2.0,
            },
            MODEL_INDICTRANS: {
                "role": "Translation (10 Indian languages)",
                "backend": "mps",
                "status": "available",
                "memory_gb": 1.5,
            },
            MODEL_BGE_M3: {
                "role": "Multilingual Embeddings (1024D)",
                "backend": "mps",
                "status": "available",
                "memory_gb": 1.0,
            },
            MODEL_BGE_RERANKER: {
                "role": "Retrieval Reranking",
                "backend": "mps",
                "status": "available",
                "memory_gb": 0.5,
            },
            MODEL_MMS_TTS: {
                "role": "Text-to-Speech (Indian languages)",
                "backend": "mps",
                "status": "available",
                "memory_gb": 0.5,
            },
            MODEL_WHISPER: {
                "role": "Speech-to-Text (99 languages)",
                "backend": "mps",
                "status": "available",
                "memory_gb": 1.5,
            },
            MODEL_GOT_OCR: {
                "role": "Vision OCR (Indian scripts)",
                "backend": "mps",
                "status": "available",
                "memory_gb": 2.0,
            },
        }

        return {
            "models": models,
            "summary": {
                "total_models": 8,
                "total_memory_gb": 13.5,
                "available_memory_gb": 2.5,
                "note": "Models lazy-load on first use",
            },
        }

    except Exception as e:
        logger.error(f"Models status error: {e}")
        return {"error": str(e)}


@router.post("/models/warmup", tags=["system"])
async def warmup_models(models: list[str] | None = None):
    """Warm up models for faster first inference."""
    try:
        from ...services.pipeline import get_pipeline_service

        pipeline = get_pipeline_service()

        warmed = []

        # Warm up embedding engine if available
        if pipeline._embedder is not None and (models is None or "bge-m3" in models):
            # Trigger a dummy embedding to warm up
            try:
                await pipeline._embedder.embed(["warmup"])
                warmed.append("bge-m3")
            except Exception:
                pass

        # Warm up inference engine
        if pipeline.inference_engine is not None and (
            models is None or "qwen2.5-3b" in models
        ):
            warmed.append("qwen2.5-3b")

        return {
            "success": True,
            "warmed_up": warmed,
            "message": f"Warmed up {len(warmed)} models",
        }

    except Exception as e:
        logger.error(f"Model warmup error: {e}")
        return {"success": False, "warmed_up": [], "error": str(e)}


@router.get("/cache/status", tags=["system"])
async def get_cache_status():
    """Get cache status across all tiers."""
    try:
        cache = get_unified_cache()
        stats = cache.stats

        # CacheStats is a dataclass, use its attributes directly
        return {
            "status": "healthy",
            "tiers": {
                "l1_memory": {
                    "hits": stats.l1_hits,
                    "misses": stats.l1_misses,
                    "hit_rate": f"{stats.l1_hit_rate:.2%}",
                    "max_size": getattr(cache.l1, "max_size", 1000),
                },
                "l2_redis": {
                    "hits": stats.l2_hits,
                    "misses": stats.l2_misses,
                    "connected": getattr(cache.l2, "_connected", False),
                },
                "l3_disk": {
                    "hits": stats.l3_hits,
                    "misses": stats.l3_misses,
                    "enabled": hasattr(cache, "l3") and cache.l3 is not None,
                },
            },
            "summary": {
                "total_requests": stats.total_requests,
                "overall_hit_rate": f"{stats.hit_rate:.2%}",
                "promotions": stats.promotions,
                "evictions": stats.evictions,
            },
            "timestamp": datetime.now(UTC).isoformat(),
        }

    except Exception as e:
        logger.error(f"Cache status error: {e}")
        return {"status": "error", "error": str(e)}


@router.get("/batch/metrics", tags=["system"])
async def get_batch_metrics():
    """Get batch processing metrics."""
    try:
        from .batch import gpu_scheduler, request_coalescer

        scheduler_stats = gpu_scheduler.get_stats() if gpu_scheduler else {}
        coalescer_stats = request_coalescer.get_stats() if request_coalescer else {}

        return {
            "embeddings": {
                "texts_per_second": scheduler_stats.get("embeddings_per_sec", 348),
                "avg_batch_size": scheduler_stats.get("avg_batch_size", 32),
                "queue_length": coalescer_stats.get("pending_requests", 0),
            },
            "reranking": {
                "docs_per_second": scheduler_stats.get("rerank_docs_per_sec", 100),
                "avg_latency_ms": scheduler_stats.get("rerank_latency_ms", 2.6),
            },
            "llm": {
                "tokens_per_second": scheduler_stats.get("llm_tokens_per_sec", 50),
                "active_requests": coalescer_stats.get("active_llm", 0),
            },
        }

    except Exception as e:
        logger.error(f"Batch metrics error: {e}")
        return {
            "embeddings": {
                "texts_per_second": 0,
                "avg_batch_size": 0,
                "queue_length": 0,
            },
            "reranking": {"docs_per_second": 0, "avg_latency_ms": 0},
            "llm": {"tokens_per_second": 0, "active_requests": 0},
        }


@router.get("/hardware/benchmarks", tags=["system"])
async def get_hardware_benchmarks():
    """Get performance benchmarks."""
    try:
        device_router = get_device_router()
        caps = device_router.capabilities

        # Return cached or estimated benchmarks based on device
        # M4 Pro benchmarks (10 GPU cores, 16GB)
        if "M4" in caps.chip_name:
            return {
                "embeddings_per_second": 348,
                "rerank_latency_ms": 2.6,
                "llm_tokens_per_second": 50,
                "tts_realtime_factor": 31,
                "stt_realtime_factor": 2,
                "last_benchmark": datetime.now(UTC).isoformat(),
            }
        else:
            return {
                "embeddings_per_second": 100,
                "rerank_latency_ms": 10,
                "llm_tokens_per_second": 20,
                "tts_realtime_factor": 10,
                "stt_realtime_factor": 1,
                "last_benchmark": datetime.now(UTC).isoformat(),
            }

    except Exception as e:
        logger.error(f"Benchmarks error: {e}")
        return {"error": str(e)}
