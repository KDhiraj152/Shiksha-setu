"""Text Simplifier using Qwen2.5-3B-Instruct for grade-level content adaptation.

Optimal 2025 Model Stack: Qwen2.5-3B-Instruct
- Best instruction-following for educational prompts
- 3GB with INT4 quantization
- Supports vLLM serving for production

v1.4.0: M4-optimized semantic accuracy refinement (target 9.0+)
- 5-Phase hardware optimization (async, cache, GPU, cores, memory)
- Iterative refinement loop with task-aware evaluation
- Multi-dimensional scoring (factual accuracy, educational clarity, etc.)
- Automatic regeneration when score < 9.0
- Achieves 80-150 tok/s on Apple M4
"""

import logging
import re
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Any, Dict, List, Optional

import httpx

from backend.core.config import settings

# M4 Hardware optimization imports
try:
    from backend.core.optimized import get_affinity_manager, get_memory_pool
    from backend.core.optimized.device_router import (
        TaskType,
        get_device_router,
        get_resource_manager,
    )
    from backend.core.optimized.quantization import QuantizationStrategy

    HARDWARE_OPT_AVAILABLE = True
except ImportError:
    HARDWARE_OPT_AVAILABLE = False

# Refinement pipeline for semantic accuracy
try:
    from backend.services.evaluation.refinement_pipeline import (
        RefinementConfig,
        RefinementTask,
        SemanticRefinementPipeline,
    )

    REFINEMENT_AVAILABLE = True
except ImportError:
    REFINEMENT_AVAILABLE = False
    # Placeholder classes for test patching
    RefinementConfig = None
    RefinementTask = None
    SemanticRefinementPipeline = None
    logger_temp = logging.getLogger(__name__)
    logger_temp.warning(
        "Refinement pipeline not available, using single-pass generation"
    )

logger = logging.getLogger(__name__)


@dataclass
class SimplifiedText:
    """Result of text simplification."""

    text: str
    complexity_score: float
    grade_level: int | None  # Now optional - None for unconstrained
    subject: str
    metadata: dict[str, Any]

    # Refinement metrics (v1.3.4)
    semantic_score: float | None = None
    refinement_iterations: int = 0
    dimension_scores: dict[str, float] | None = None


class BaseLLMClient(ABC):
    """Abstract base class for LLM clients."""

    @abstractmethod
    async def generate(self, prompt: str, **kwargs) -> str:
        """Generate text from prompt."""
        pass


class VLLMClient(BaseLLMClient):
    """vLLM OpenAI-compatible API client."""

    def __init__(
        self,
        base_url: str | None = None,
        model: str | None = None,
        api_key: str | None = None,
    ):
        self.base_url = base_url or settings.vllm_base_url
        self.model = model or settings.SIMPLIFICATION_MODEL_ID
        self.api_key = api_key or settings.VLLM_API_KEY
        self.client = httpx.AsyncClient(
            timeout=120.0
        )  # Increased for longer generations

    async def generate(
        self,
        prompt: str,
        max_tokens: int = 4096,  # Increased for complete responses
        temperature: float = 0.7,
        **kwargs,
    ) -> str:
        """Generate text using vLLM OpenAI-compatible API."""
        headers = {"Content-Type": "application/json"}
        if self.api_key:
            headers["Authorization"] = f"Bearer {self.api_key}"

        payload = {
            "model": self.model,
            "messages": [
                {
                    "role": "system",
                    "content": "You are an expert educational content simplifier for Indian students.",
                },
                {"role": "user", "content": prompt},
            ],
            "max_tokens": max_tokens,
            "temperature": temperature,
        }

        try:
            response = await self.client.post(
                f"{self.base_url}/chat/completions", json=payload, headers=headers
            )
            response.raise_for_status()
            data = response.json()
            return data["choices"][0]["message"]["content"]
        except Exception as e:
            logger.error(f"vLLM generation failed: {e}")
            raise

    async def close(self):
        await self.client.aclose()


class TransformersClient(BaseLLMClient):
    """Direct transformers inference client optimized for Apple Silicon M4.

    Optimization Strategy:
    - MPS (Metal Performance Shaders) for GPU acceleration on M4
    - float16 for memory efficiency on unified memory
    - Proper memory management for 16GB unified RAM
    - Thread optimization for M4's 4P+6E core layout
    """

    def __init__(self, model_id: str | None = None):
        self.model_id = model_id or settings.SIMPLIFICATION_MODEL_ID
        self._model = None
        self._tokenizer = None
        self._device = None

    def _select_device(self) -> str:
        """Select optimal device for inference using hardware optimizer."""
        import torch

        # Use hardware optimizer for intelligent device routing if available
        if HARDWARE_OPT_AVAILABLE:
            try:
                router = get_device_router()
                routing = router.route(TaskType.LLM_INFERENCE)
                logger.info(
                    f"TransformersClient: Using {routing.device_str} (via hardware optimizer, speedup: {routing.estimated_speedup}x)"
                )
                return routing.device_str
            except Exception as e:
                logger.debug(f"Hardware optimizer failed, using fallback: {e}")

        # Fallback manual detection
        if torch.cuda.is_available():
            return "cuda"
        if hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
            return "mps"
        return "cpu"

    def _get_load_kwargs(self, device: str):
        """Get model loading configuration based on device."""
        import torch

        from ...core.config import settings

        load_kwargs = {
            "low_cpu_mem_usage": True,
            "torch_dtype": torch.float16 if device != "cpu" else torch.float32,
            "cache_dir": str(settings.MODEL_CACHE_DIR),
        }

        if device == "cuda":
            load_kwargs["device_map"] = "auto"
            if settings.USE_QUANTIZATION:
                try:
                    from transformers import BitsAndBytesConfig

                    load_kwargs["quantization_config"] = BitsAndBytesConfig(
                        load_in_4bit=True,
                        bnb_4bit_compute_dtype=torch.float16,
                    )
                except ImportError:
                    logger.warning(
                        "bitsandbytes not available, loading without quantization"
                    )

        return load_kwargs

    def _configure_threading(self):
        """Configure thread settings for optimal M4 performance."""
        import os

        import torch

        torch.set_num_threads(4)
        os.environ["OMP_NUM_THREADS"] = "4"
        os.environ["MKL_NUM_THREADS"] = "4"

    def _load_model(self):
        """Lazy load the model with M4-optimized settings."""
        if self._model is not None:
            return

        import os

        import torch
        from transformers import AutoModelForCausalLM, AutoTokenizer

        logger.info(f"Loading model: {self.model_id}")

        # Configure threading for M4 (4 performance + 6 efficiency cores)
        self._configure_threading()

        # Select device and configure MPS fallback if needed
        self._device = self._select_device()
        if self._device == "mps":
            os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

        logger.info(f"Selected device: {self._device}")

        # Get device-specific loading configuration
        load_kwargs = self._get_load_kwargs(self._device)

        # Load tokenizer and model
        from ...core.config import settings

        self._tokenizer = AutoTokenizer.from_pretrained(
            self.model_id, use_fast=True, cache_dir=str(settings.MODEL_CACHE_DIR)
        )
        self._model = AutoModelForCausalLM.from_pretrained(self.model_id, **load_kwargs)

        # Move to device (MPS or CPU - CUDA uses device_map)
        if self._device in ("mps", "cpu") and load_kwargs.get("device_map") is None:
            self._model = self._model.to(self._device)

        # Set eval mode and clear MPS cache
        self._model.eval()
        if self._device == "mps":
            torch.mps.empty_cache()

        logger.info(
            f"Model loaded on {self._device} (dtype: {load_kwargs['torch_dtype']})"
        )

    async def generate(
        self,
        prompt: str,
        max_tokens: int = 4096,  # Increased for complete responses
        temperature: float = 0.7,
        **kwargs,
    ) -> str:
        """Generate text using transformers with M4-optimized inference.

        Model loading is performed in a thread pool to avoid blocking the event loop.
        """
        import asyncio

        import torch

        # Load model in thread pool to avoid blocking async event loop
        loop = asyncio.get_running_loop()
        await loop.run_in_executor(None, self._load_model)

        # Format as chat template for instruction-tuned model
        messages = [
            {
                "role": "system",
                "content": "You are an expert educational content simplifier for Indian students.",
            },
            {"role": "user", "content": prompt},
        ]

        # Apply chat template
        if hasattr(self._tokenizer, "apply_chat_template"):
            formatted_prompt = self._tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
        else:
            formatted_prompt = f"System: You are an expert educational content simplifier.\n\nUser: {prompt}\n\nAssistant:"

        # Tokenize with truncation for memory safety
        inputs = self._tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=8192,  # Increased for longer prompts (Qwen2.5 supports this)
        )
        inputs = {k: v.to(self._model.device) for k, v in inputs.items()}

        # =============================================================
        # ADVANCED PERFORMANCE OPTIMIZATIONS
        # =============================================================

        # 1. Compute optimal max_tokens based on input length
        #    Adaptive: longer inputs can still get substantial output
        input_len = inputs["input_ids"].shape[1]
        # Dynamic allocation: use model's full context window efficiently
        # Cap at 6144 or passed max_tokens, minimum 512 tokens for meaningful output
        adaptive_max_tokens = min(max_tokens, 6144, max(512, 8192 - input_len))

        # 2. Build generation config with all optimizations
        generation_config = {
            "max_new_tokens": adaptive_max_tokens,
            "use_cache": True,  # KV cache is critical for speed
            "pad_token_id": self._tokenizer.eos_token_id,
            "eos_token_id": self._tokenizer.eos_token_id,
            # Early stopping on multiple EOS patterns - requires tokenizer
            "stop_strings": ["</s>", "<|endoftext|>", "\n\n---\n"],
            "tokenizer": self._tokenizer,  # Required for stop_strings to work
        }

        # 3. Choose sampling strategy based on temperature
        #    Greedy (temp < 0.1) is 20-30% faster than sampling
        if temperature <= 0.1:
            generation_config["do_sample"] = False
            generation_config["num_beams"] = 1  # Greedy decoding
        else:
            # Use passed parameters or defaults
            # Lower temperature + higher repetition_penalty = more coherent Indian language output
            repetition_penalty = kwargs.get("repetition_penalty", 1.1)
            top_p = kwargs.get("top_p", 0.9)
            top_k = kwargs.get("top_k", 50)

            generation_config.update(
                {
                    "do_sample": True,
                    "temperature": max(
                        0.1, min(temperature, 1.5)
                    ),  # Clamp temperature to safe range
                    "top_p": top_p,
                    "top_k": top_k,  # Limit vocabulary for faster sampling
                    "repetition_penalty": repetition_penalty,
                }
            )

        # 4. Run inference in thread pool to avoid blocking event loop
        def _run_inference():
            with torch.inference_mode():
                try:
                    return self._model.generate(**inputs, **generation_config)
                except RuntimeError as e:
                    # Handle numerical instability by falling back to greedy decoding
                    if (
                        "probability tensor" in str(e)
                        or "inf" in str(e)
                        or "nan" in str(e)
                    ):
                        logger.warning(f"Sampling failed, falling back to greedy: {e}")
                        fallback_config = {
                            "max_new_tokens": adaptive_max_tokens,
                            "use_cache": True,
                            "pad_token_id": self._tokenizer.eos_token_id,
                            "eos_token_id": self._tokenizer.eos_token_id,
                            "do_sample": False,
                            "num_beams": 1,
                        }
                        return self._model.generate(**inputs, **fallback_config)
                    raise

        outputs = await loop.run_in_executor(None, _run_inference)

        # 5. Decode only the new tokens (skip input)
        generated_text = self._tokenizer.decode(
            outputs[0][inputs["input_ids"].shape[1] :], skip_special_tokens=True
        )

        # 6. Early termination cleanup (remove trailing incomplete sentences)
        #    Improves quality when max_tokens cuts mid-sentence
        generated_text = self._clean_incomplete_ending(generated_text)

        return generated_text.strip()

    def _clean_incomplete_ending(self, text: str) -> str:
        """Remove incomplete trailing sentence for cleaner output."""
        if not text:
            return text

        # If ends with proper punctuation, keep as-is
        if text.rstrip()[-1] in ".!?ред":
            return text

        # Find last complete sentence
        for punct in [". ", "! ", "? ", "ред "]:
            last_idx = text.rfind(punct)
            if last_idx > len(text) * 0.7:  # Only trim if near end
                return text[: last_idx + 1]

        return text


class TextSimplifier:
    """
    Text simplification using Qwen2.5-3B-Instruct.

    Adapts educational content complexity based on grade level (5-12)
    using state-of-the-art instruction-following model.
    """

    # Grade level ranges
    ELEMENTARY_GRADES = range(5, 7)  # Grades 5-6
    MIDDLE_GRADES = range(7, 9)  # Grades 7-8
    SECONDARY_GRADES = range(9, 13)  # Grades 9-12

    # Complexity thresholds
    COMPLEXITY_THRESHOLDS = {
        "elementary": (0.0, 0.4),
        "middle": (0.4, 0.7),
        "secondary": (0.7, 1.0),
    }

    # Subject terminology to preserve
    SUBJECT_TERMINOLOGY = {
        "Mathematics": [
            "equation",
            "variable",
            "coefficient",
            "polynomial",
            "theorem",
            "proof",
            "derivative",
            "integral",
            "function",
            "graph",
        ],
        "Science": [
            "photosynthesis",
            "molecule",
            "atom",
            "cell",
            "organism",
            "energy",
            "force",
            "velocity",
            "acceleration",
            "chemical",
        ],
        "Social Studies": [
            "democracy",
            "constitution",
            "government",
            "economy",
            "culture",
        ],
        "History": ["ancient", "medieval", "modern", "empire", "dynasty"],
        "Geography": ["latitude", "longitude", "climate", "topography", "ecosystem"],
    }

    def __init__(
        self,
        client: BaseLLMClient = None,
        enable_refinement: bool = True,
        target_semantic_score: float = 9.0,  # M4-optimized target
    ):
        """
        Initialize Text Simplifier.

        Args:
            client: LLM client (vLLM or transformers). Auto-selects based on config.
            enable_refinement: Enable iterative refinement for semantic accuracy (default: True)
            target_semantic_score: Minimum semantic score target (default: 9.0 for M4)
        """
        if client:
            self.client = client
        elif settings.VLLM_ENABLED and settings.SIMPLIFICATION_BACKEND == "vllm":
            self.client = VLLMClient()
        else:
            self.client = TransformersClient()

        # Refinement configuration (v1.3.4)
        self.enable_refinement = enable_refinement and REFINEMENT_AVAILABLE
        self.target_semantic_score = target_semantic_score
        self._refinement_pipeline = None

        if self.enable_refinement:
            logger.info(
                f"TextSimplifier with refinement (target: {target_semantic_score})"
            )
        else:
            logger.info(f"TextSimplifier initialized with {type(self.client).__name__}")

    def _get_refinement_pipeline(self) -> "SemanticRefinementPipeline":
        """Lazy-load refinement pipeline to avoid circular imports."""
        if self._refinement_pipeline is None and REFINEMENT_AVAILABLE:
            config = RefinementConfig(
                target_score=self.target_semantic_score, max_iterations=3
            )
            self._refinement_pipeline = SemanticRefinementPipeline(config=config)
        return self._refinement_pipeline

    async def simplify_text(
        self,
        content: str,
        grade_level: int | None = None,
        subject: str = "General",
        use_refinement: bool | None = None,
    ) -> SimplifiedText:
        """
        Simplify text content with optional grade level targeting.

        Args:
            content: Original text to simplify
            grade_level: Optional target grade level (5-12), None for unconstrained
            subject: Subject area for context
            use_refinement: Override refinement setting (None = use instance default)

        Returns:
            SimplifiedText with simplified content, metadata, and semantic scores
        """
        if not content or len(content.strip()) == 0:
            raise ValueError("Content cannot be empty")

        # Use middle school level as default for unconstrained simplification
        effective_grade = grade_level if grade_level is not None else 8

        logger.info(f"Simplifying text for grade {effective_grade}, subject {subject}")

        # Calculate original complexity
        original_complexity = self.get_complexity_score(content)
        target_range = self._get_target_complexity_range(effective_grade)

        # Determine if refinement should be used
        should_refine = (
            use_refinement if use_refinement is not None else self.enable_refinement
        )

        if should_refine and REFINEMENT_AVAILABLE:
            return await self._simplify_with_refinement(
                content, effective_grade, subject, original_complexity, target_range
            )
        else:
            return await self._simplify_single_pass(
                content, effective_grade, subject, original_complexity, target_range
            )

    async def _simplify_single_pass(
        self,
        content: str,
        grade_level: int,
        subject: str,
        original_complexity: float,
        target_range: tuple,
    ) -> SimplifiedText:
        """Single-pass simplification without refinement (original behavior)."""
        prompt = self._create_qwen_prompt(content, grade_level, subject)

        try:
            simplified_content = await self.client.generate(
                prompt,
                max_tokens=settings.SIMPLIFICATION_MAX_LENGTH,
                temperature=settings.SIMPLIFICATION_TEMPERATURE,
            )
            method = "qwen2.5-3b"
        except Exception as e:
            logger.warning(f"LLM generation failed: {e}, using rule-based fallback")
            simplified_content = self._rule_based_simplification(content, grade_level)
            method = "rule_based"

        final_complexity = self.get_complexity_score(simplified_content)

        return SimplifiedText(
            text=simplified_content,
            complexity_score=final_complexity,
            grade_level=grade_level,
            subject=subject,
            metadata={
                "original_complexity": original_complexity,
                "target_complexity_range": target_range,
                "simplification_method": method,
                "model": settings.SIMPLIFICATION_MODEL_ID,
                "refinement_enabled": False,
            },
        )

    async def _simplify_with_refinement(
        self,
        content: str,
        grade_level: int,
        subject: str,
        original_complexity: float,
        target_range: tuple,
    ) -> SimplifiedText:
        """
        Simplify with iterative refinement to achieve target semantic accuracy.

        This uses the SemanticRefinementPipeline to:
        1. Generate initial simplification
        2. Evaluate semantic accuracy across multiple dimensions
        3. If score < 8.2, pipeline refines with feedback
        4. Repeat until target reached or max iterations hit
        """
        # First, generate initial simplification
        prompt = self._create_qwen_prompt(content, grade_level, subject)

        try:
            initial_output = await self.client.generate(
                prompt,
                max_tokens=settings.SIMPLIFICATION_MAX_LENGTH,
                temperature=settings.SIMPLIFICATION_TEMPERATURE,
            )
            method = "qwen2.5-3b"
        except Exception as e:
            logger.warning(f"Initial generation failed: {e}, using rule-based")
            initial_output = self._rule_based_simplification(content, grade_level)
            method = "rule_based"

        # Run refinement pipeline
        try:
            pipeline = self._get_refinement_pipeline()
            result = await pipeline.refine(
                original_text=content,
                initial_output=initial_output,
                task=RefinementTask.SIMPLIFICATION,
                grade_level=grade_level,
                subject=subject,
            )

            final_complexity = self.get_complexity_score(result.final_text)

            return SimplifiedText(
                text=result.final_text,
                complexity_score=final_complexity,
                grade_level=grade_level,
                subject=subject,
                metadata={
                    "original_complexity": original_complexity,
                    "target_complexity_range": target_range,
                    "simplification_method": f"{method}-refined",
                    "model": settings.SIMPLIFICATION_MODEL_ID,
                    "refinement_enabled": True,
                    "target_reached": result.achieved_target,
                    "initial_score": result.iteration_history[0].score
                    if result.iteration_history
                    else 0,
                    "iterations_history": [
                        {"iteration": it.iteration, "score": it.score}
                        for it in result.iteration_history
                    ],
                },
                semantic_score=result.final_score,
                refinement_iterations=result.iterations_used,
                dimension_scores=dict(result.iteration_history[-1].dimension_scores)
                if result.iteration_history
                else None,
            )

        except Exception as e:
            logger.error(f"Refinement pipeline failed: {e}, returning initial output")
            final_complexity = self.get_complexity_score(initial_output)
            return SimplifiedText(
                text=initial_output,
                complexity_score=final_complexity,
                grade_level=grade_level,
                subject=subject,
                metadata={
                    "original_complexity": original_complexity,
                    "target_complexity_range": target_range,
                    "simplification_method": method,
                    "model": settings.SIMPLIFICATION_MODEL_ID,
                    "refinement_enabled": True,
                    "refinement_error": str(e),
                },
            )

    def _create_qwen_prompt(self, content: str, grade_level: int, subject: str) -> str:
        """Create optimized prompt for Qwen2.5-3B-Instruct."""

        # Grade-specific instructions
        if grade_level in self.ELEMENTARY_GRADES:
            level_desc = "elementary school students (grades 5-6)"
            style = """
- Use very simple sentences (10-15 words max)
- Replace difficult words with everyday alternatives
- Add helpful examples from daily life
- Break complex ideas into small, digestible parts"""
        elif grade_level in self.MIDDLE_GRADES:
            level_desc = "middle school students (grades 7-8)"
            style = """
- Use clear, straightforward language
- Explain technical terms when first used
- Provide relevant examples
- Maintain logical flow between ideas"""
        else:
            level_desc = "high school students (grades 9-12)"
            style = """
- Maintain academic rigor while ensuring clarity
- Use appropriate terminology with brief explanations
- Present concepts with supporting evidence
- Keep sophisticated structure but improve readability"""

        # Subject-specific terms to preserve
        preserve_terms = self.SUBJECT_TERMINOLOGY.get(subject, [])
        terms_note = ""
        if preserve_terms:
            terms_note = f"\n\nIMPORTANT: Preserve these key {subject} terms (explain if needed): {', '.join(preserve_terms[:5])}"

        prompt = f"""Simplify the following {subject} educational content for {level_desc}.

REQUIREMENTS:
{style}{terms_note}

ORIGINAL TEXT:
{content}

SIMPLIFIED VERSION:"""

        return prompt

    def _get_target_complexity_range(self, grade_level: int) -> tuple:
        """Get target complexity range for grade level."""
        if grade_level in self.ELEMENTARY_GRADES:
            return self.COMPLEXITY_THRESHOLDS["elementary"]
        elif grade_level in self.MIDDLE_GRADES:
            return self.COMPLEXITY_THRESHOLDS["middle"]
        return self.COMPLEXITY_THRESHOLDS["secondary"]

    def get_complexity_score(self, text: str) -> float:
        """
        Calculate text complexity score (0-1).

        Uses readability metrics:
        - Average sentence length
        - Average word length
        - Syllable count
        """
        if not text or len(text.strip()) == 0:
            return 0.0

        sentences = self._split_sentences(text)
        words = self._split_words(text)

        if not sentences or not words:
            return 0.0

        avg_sentence_len = len(words) / len(sentences)
        avg_word_len = sum(len(w) for w in words) / len(words)
        avg_syllables = sum(self._count_syllables(w) for w in words) / len(words)

        # Normalize to 0-1 scale
        sentence_score = min(max((avg_sentence_len - 5) / 25, 0), 1)
        word_score = min(max((avg_word_len - 3) / 7, 0), 1)
        syllable_score = min(max((avg_syllables - 1) / 3, 0), 1)

        return 0.4 * sentence_score + 0.3 * word_score + 0.3 * syllable_score

    def _split_sentences(self, text: str) -> list[str]:
        """Split text into sentences."""
        sentences = re.split(r"[.!?]+", text)
        return [s.strip() for s in sentences if s.strip()]

    def _split_words(self, text: str) -> list[str]:
        """Split text into words."""
        words = re.findall(r"\b[a-zA-Z]+\b", text.lower())
        return words

    def _count_syllables(self, word: str) -> int:
        """Count syllables in a word (approximate)."""
        word = word.lower()
        if len(word) <= 3:
            return 1

        vowels = "aeiou"
        count = 0
        prev_vowel = False

        for char in word:
            is_vowel = char in vowels
            if is_vowel and not prev_vowel:
                count += 1
            prev_vowel = is_vowel

        # Adjust for silent 'e'
        if word.endswith("e"):
            count -= 1

        return max(count, 1)

    def _rule_based_simplification(self, text: str, grade_level: int) -> str:
        """Fallback rule-based simplification."""
        # Simple transformations
        result = text

        # Replace complex words (basic dictionary)
        replacements = {
            "utilize": "use",
            "approximately": "about",
            "demonstrate": "show",
            "consequently": "so",
            "furthermore": "also",
            "nevertheless": "but",
            "subsequently": "then",
            "therefore": "so",
            "regarding": "about",
            "numerous": "many",
            "sufficient": "enough",
            "commence": "start",
            "terminate": "end",
            "endeavor": "try",
            "facilitate": "help",
        }

        for complex_word, simple_word in replacements.items():
            result = re.sub(
                rf"\b{complex_word}\b", simple_word, result, flags=re.IGNORECASE
            )

        # For lower grades, break long sentences
        if grade_level <= 7:
            result = self._break_long_sentences(result)

        return result

    def _break_long_sentences(self, text: str) -> str:
        """Break long sentences into shorter ones."""
        sentences = self._split_sentences(text)
        result = []
        conjunction_pattern = re.compile(
            r"\b(and|but|or|because|so|while|when)\b", flags=re.IGNORECASE
        )
        conjunctions = {"and", "but", "or", "because", "so", "while", "when"}

        for sentence in sentences:
            if len(sentence.split()) <= 25:
                result.append(sentence)
                continue

            # Split long sentences at conjunctions
            parts = conjunction_pattern.split(sentence)
            for part in parts:
                part = part.strip()
                if part and part.lower() not in conjunctions:
                    result.append(part.capitalize() if not part[0].isupper() else part)

        return ". ".join(result) + "."


# Synchronous wrapper for backward compatibility
def simplify_text_sync(
    content: str,
    grade_level: int,
    subject: str,
    simplifier: TextSimplifier = None,
    use_refinement: bool = True,
) -> SimplifiedText:
    """Synchronous wrapper for text simplification with refinement."""
    import asyncio

    if simplifier is None:
        simplifier = TextSimplifier(enable_refinement=use_refinement)

    return asyncio.run(simplifier.simplify_text(content, grade_level, subject))


# Export
__all__ = [
    "REFINEMENT_AVAILABLE",
    "SimplifiedText",
    "TextSimplifier",
    "TransformersClient",
    "VLLMClient",
    "simplify_text_sync",
]
