"""
Unified Model Client for all AI/ML inference tasks.

Consolidates FlanT5Client, IndicTrans2Client, BERTClient, VITSClient into
a single async-first implementation with:
- Lazy loading + LRU caching
- Quantization support (4-bit/8-bit)
- MPS/CUDA/CPU optimization
- Circuit breaker + API fallback
- Model tier routing
"""
import asyncio
import logging
from typing import Dict, Any, Optional, List, Union
from enum import Enum

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM

from ..core.config import settings
from ..core.model_loader import LazyModelLoader
from ..core.model_tier_router import get_router, ModelTier
from ..utils.device_manager import get_device_manager
from ..utils.circuit_breaker import circuit_breaker

logger = logging.getLogger(__name__)


class TaskType(str, Enum):
    """Supported inference task types."""
    SIMPLIFICATION = "simplification"
    TRANSLATION = "translation"
    VALIDATION = "validation"
    EMBEDDING = "embedding"
    TTS = "tts"


class UnifiedModelClient:
    """
    Unified client for all model inference tasks.
    
    Features:
    - Single interface for all model types
    - Automatic tier selection (SMALL/MEDIUM/LARGE)
    - Lazy loading with LRU caching
    - Quantization for memory efficiency
    - Circuit breaker with API fallback
    - Async-first design
    """
    
    def __init__(self):
        """Initialize unified model client."""
        self.device_manager = get_device_manager()
        self.model_loader = LazyModelLoader(
            models_dir=str(settings.MODEL_CACHE_DIR),
            max_cache_size_mb=int(settings.MAX_MODEL_MEMORY_GB * 1024)
        )
        self.router = get_router()
        
        # Loaded models cache (model_id -> (model, tokenizer))
        self._models_cache: Dict[str, tuple] = {}
        
        logger.info(
            f"UnifiedModelClient initialized: device={self.device_manager.device}, "
            f"max_memory={settings.MAX_MODEL_MEMORY_GB}GB"
        )
    
    async def simplify_text(
        self,
        text: str,
        grade_level: int,
        subject: str,
        force_tier: Optional[ModelTier] = None
    ) -> str:
        """
        Simplify text for target grade level.
        
        Args:
            text: Input text to simplify
            grade_level: Target grade level (5-12)
            subject: Subject area
            force_tier: Force specific model tier
            
        Returns:
            Simplified text
        """
        # Route to appropriate tier
        tier, config, complexity = self.router.route_task(
            text=text,
            grade_level=grade_level,
            subject=subject,
            force_tier=force_tier
        )
        
        if tier == ModelTier.API:
            # Fallback to API
            return await self._simplify_via_api(text, grade_level, subject)
        
        # Load model for selected tier
        model_id = config["model_id"]
        model, tokenizer = await self._get_or_load_model(
            model_id=model_id,
            quantization=config["quantization"],
            task_type=TaskType.SIMPLIFICATION
        )
        
        # Create prompt
        prompt = self._create_simplification_prompt(text, grade_level, subject)
        
        # Run inference
        try:
            result = await self._run_inference(
                model=model,
                tokenizer=tokenizer,
                prompt=prompt,
                max_tokens=config["max_tokens"]
            )
            
            logger.info(
                f"Simplified text: tier={tier.value}, model={model_id}, "
                f"input_len={len(text)}, output_len={len(result)}"
            )
            
            return result
            
        except Exception as e:
            logger.error(f"Simplification failed: {e}")
            # Fallback to API
            return await self._simplify_via_api(text, grade_level, subject)
    
    async def translate_text(
        self,
        text: str,
        source_lang: str,
        target_lang: str,
        force_tier: Optional[ModelTier] = None
    ) -> str:
        """
        Translate text between languages.
        
        Args:
            text: Input text
            source_lang: Source language code (e.g., 'en')
            target_lang: Target language code (e.g., 'hi')
            force_tier: Force specific tier
            
        Returns:
            Translated text
        """
        # Route based on text length
        tier, config, _ = self.router.route_task(
            text=text,
            grade_level=8,  # Default for translation
            subject="General",
            language_count=2,
            force_tier=force_tier
        )
        
        if tier == ModelTier.API:
            return await self._translate_via_api(text, source_lang, target_lang)
        
        # Use IndicTrans2 for Indian languages
        model_id = "ai4bharat/indictrans2-en-indic-1B"
        model, tokenizer = await self._get_or_load_model(
            model_id=model_id,
            quantization=settings.INDICTRANS2_QUANTIZATION,
            task_type=TaskType.TRANSLATION
        )
        
        # Create translation prompt
        prompt = f"Translate from {source_lang} to {target_lang}: {text}"
        
        try:
            result = await self._run_inference(
                model=model,
                tokenizer=tokenizer,
                prompt=prompt,
                max_tokens=len(text) * 2  # Allow expansion
            )
            
            logger.info(f"Translated: {source_lang}->{target_lang}, tier={tier.value}")
            return result
            
        except Exception as e:
            logger.error(f"Translation failed: {e}")
            return await self._translate_via_api(text, source_lang, target_lang)
    
    async def _get_or_load_model(
        self,
        model_id: str,
        quantization: str,
        task_type: TaskType
    ) -> tuple:
        """
        Get model from cache or load lazily.
        
        Args:
            model_id: HuggingFace model ID
            quantization: Quantization type
            task_type: Task type
            
        Returns:
            (model, tokenizer) tuple
        """
        cache_key = f"{model_id}_{quantization}"
        
        if cache_key in self._models_cache:
            logger.debug(f"Model cache hit: {cache_key}")
            return self._models_cache[cache_key]
        
        logger.info(f"Loading model: {model_id} ({quantization})")
        
        # Load tokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_id)
        
        # Load model with quantization
        model = self.model_loader.load_quantized_model(
            model_id=model_id,
            quantization=quantization,
            device=self.device_manager.device_str
        )
        
        # Cache it
        self._models_cache[cache_key] = (model, tokenizer)
        
        # Update router memory tracking
        if quantization == "4bit":
            size_gb = 3.5  # Rough estimate for 7B in 4-bit
        elif quantization == "8bit":
            size_gb = 7.0
        else:
            size_gb = 14.0  # FP16
        
        self.router.update_memory_usage(
            tier=ModelTier.MEDIUM,  # Assume medium for now
            loaded=True
        )
        
        return model, tokenizer
    
    async def _run_inference(
        self,
        model: Any,
        tokenizer: Any,
        prompt: str,
        max_tokens: int = 512
    ) -> str:
        """
        Run model inference asynchronously.
        
        Args:
            model: Loaded model
            tokenizer: Tokenizer
            prompt: Input prompt
            max_tokens: Max output tokens
            
        Returns:
            Generated text
        """
        # Tokenize
        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            max_length=2048,
            truncation=True
        ).to(model.device)
        
        # Run inference in thread pool (blocking operation)
        loop = asyncio.get_event_loop()
        
        def _generate():
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    pad_token_id=tokenizer.eos_token_id
                )
            return tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        result = await loop.run_in_executor(None, _generate)
        
        # Clear cache after inference
        self.device_manager.empty_cache()
        
        return result
    
    def _create_simplification_prompt(
        self,
        text: str,
        grade_level: int,
        subject: str
    ) -> str:
        """Create prompt for text simplification."""
        return (
            f"Simplify the following {subject} text for a grade {grade_level} student. "
            f"Use simple words and short sentences:\n\n{text}\n\nSimplified version:"
        )
    
    @circuit_breaker(failures=3, timeout=30)
    async def _simplify_via_api(
        self,
        text: str,
        grade_level: int,
        subject: str
    ) -> str:
        """Fallback: simplify via external API."""
        logger.warning("Using API fallback for simplification")
        
        # TODO: Implement Bhashini or other API
        # For now, return rule-based simplification
        return self._rule_based_simplification(text, grade_level)
    
    @circuit_breaker(failures=3, timeout=30)
    async def _translate_via_api(
        self,
        text: str,
        source_lang: str,
        target_lang: str
    ) -> str:
        """Fallback: translate via Bhashini API."""
        logger.warning("Using Bhashini API fallback for translation")
        
        # TODO: Implement actual Bhashini call
        # For now, return original text
        return f"[TRANSLATE {source_lang}->{target_lang}]: {text}"
    
    def _rule_based_simplification(self, text: str, grade_level: int) -> str:
        """Simple rule-based text simplification as ultimate fallback."""
        import re
        
        sentences = re.split(r'[.!?]+', text)
        simplified = []
        
        for sent in sentences:
            sent = sent.strip()
            if not sent:
                continue
            
            # Break long sentences at conjunctions for lower grades
            if grade_level <= 8 and len(sent.split()) > 20:
                parts = re.split(r'\s+(and|but|because|so|which)\s+', sent, maxsplit=1)
                if len(parts) >= 3:
                    simplified.append(parts[0].strip() + '.')
                    simplified.append(parts[2].strip() + '.')
                else:
                    simplified.append(sent + '.')
            else:
                simplified.append(sent + '.')
        
        return ' '.join(simplified)
    
    def unload_all_models(self):
        """Unload all models from cache."""
        self._models_cache.clear()
        self.device_manager.empty_cache()
        logger.info("All models unloaded from cache")


# Global client instance
_client: Optional[UnifiedModelClient] = None


def get_unified_client() -> UnifiedModelClient:
    """Get global unified model client instance."""
    global _client
    if _client is None:
        _client = UnifiedModelClient()
    return _client
